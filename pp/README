----------------------------------------------------------------------
Peak Patch MP
----------------------------------------------------------------------

Main documentation file for the Peak Patch Package

----------------------------------------------------------------------
Installation and Quick Start as of 07/09/2017:
----------------------------------------------------------------------

	0. Clone the package
	        git clone git@gitlab.com:marceloalvarez/peak-patch.git

	1. Configure the peak patch code. In the peak patch 
	   directory   	   
	        ./configure
	        source ~/.bashrc (or ~/.cshrc)
	        
	   The relevant modules to have loaded (on Niagara) are:
		1) intel/2018.2        2) intelmpi/2018.2   
		3) fftw-mpi/3.3.7      4) gsl/2.4 
		5) cfitsio/3.430       6) anaconda2/5.1.0
 


	2. Go to the peak patch directory "/src" and, if your machine
	   configuration file does not already exist (e.g. Make.mach.cita for
	   cita machines), then copy one of the existing files and edit it, 
	   e.g.
	        cd $PP_DIR/src
	        cp Make.mach.scinet-gpc Make.mach.your-machine-name
	        cp Make.mach.scinet-gpc-merge Make.mach.your-machine-name-merge
	   Now edit those files to reflect the Fortran and C compilers and
	   optimization flags you want to use. There are 2 files as one is for 
	   merge, which is not threaded and breaks on large runs if you compile 
	   with omp.

	3. Test that your installation works. Go to the example 
	   directory
	        cd $PP_DIR/example
	   Edit the first line in "param/param.params" to reflect your current
	   machine, then run
	        peak-patch.py param/param.params
	   It should take a few minutes. Be patient. You should now have a 
	   directory set up to run peak-patch. 
	   
	   You can now either submit the job to the queue with 
	        qsub <short_name>_<seed>.sh
	        eg. qsub 512Mpc_nb40_13579.sh 
	   or run yourself without submitting to the queue by:
	   
	   From the same directory type 
	        ./bin/hpkvd 1
	   To make the Homegeneous Ellipsiod collapse table, then 
	        mpirun -np 8 ./bin/hpkvd 
	   to run the job
	   
	   You will see peak-patch running, and once it is complete you will
	   have one "raw" file in output/  called
	        <run_name>_raw.pksc.<seed> 
	   To merge this file 
	        mpirun -np 8 ./bin/merge_pkvd
	        
	   This will return a final peak patch catalogue in output/ called
	        output/<run_name>_merge.pksc.<seed>
	        
	   These are the unmerged and merged halo catalogs, respectively. The merged
	   halo catalog is the final product and is the one you generally want to 
	   use
       
       OUTPUT FILES:
       This final halo catalog is a binary file with a 12 byte header, 
                header = (int Nhalo, float RTHmax, float redshiftbox)
       where Nhalo is the total number of halos found, RTHmax is the radius of
       the largest halo in the simulation, and redshiftbox is the redshift of 
       the box, which will be negative for lightcone runs.
       
       This is then followed by a list of Nhalo*11 4 byte floats that represent
                (x_halo,y_halo,z_halo,vx_halo,vy_halo,vz_halo,Rth_halo,xL_halo,yL_halo,zL_halo,delta_halo)
                
	   Sample python code to load a datafile:
	        pkfile       = open(filein,"rb")
           	Nhalo        = np.fromfile(pkfile, dtype=np.int32, count=1)[0]
            	RTHMAXin     = np.fromfile(pkfile, dtype=np.float32, count=1)[0]
            	redshiftbox  = np.fromfile(pkfile, dtype=np.float32, count=1)[0]
            	print "Nhalo = ", Nhalo

		nfloats_per_halo = 11     
            	npkdata          = nfloats_per_halo * Nhalo

            	peakdata = np.fromfile(pkfile, dtype=np.float32, count=npkdata)
            	peakdata = np.reshape(peakdata,(Nhalo,nfloats_per_halo))

            	xpos   = peakdata[:,0]
            	ypos   = peakdata[:,1]
            	zpos   = peakdata[:,2]
            	vx     = peakdata[:,3]
            	vy     = peakdata[:,4]
            	vz     = peakdata[:,5]
            	Rth    = peakdata[:,6]
            	xpos_L = peakdata[:,7]
            	ypos_L = peakdata[:,8]
            	zpos_L = peakdata[:,9]
            	deltah = peakdata[:,10]

            	Omega_M = 0.31
            	h       = 0.68
            	rho     = 2.775e11 * Omega_M * h**2
            	M       = 4.0/3*np.pi * Rth**3 * rho

	 
	 
	 4.  Make a plot of the output to see how it looks:
	        python ../python/halo_plotting/cataloguecheck output/<run_name>_merge.pksc.<seed>
   	
	 5.  You should now be able to run Peak Patch from any directory, e.g.,
	        peak-patch.py param/<parameterfile>
	     so just go to any preferred directory, 
	        cp -r $PP_DIR/example/param ./
	     and you're setup to run!
	   
	   You can find an example parameter file at
	        $PP_DIR/example/param/param.params

----------------------------------------------------------------------
Directions for designing a peak patch run:
----------------------------------------------------------------------
    
    The peak-patch parameter file is mostly self explanatory, and everything
    you'd ever want to change is commented.
    But, setting up nmesh, nbuff, and ntile can be difficult without 
    understanding the geometry of how peak-patch tiles a cubic density field.
    See http://imgur.com/vL5Vuak for image of tiling in the case ntile=2
    
    So, to set up box it is easiest to use peak-patch/tools/peak-patch-setup-run.py
    
    ie.)   python tools/peak-patch-setup-run.py <ngrid> <boxsize>
    
    This will give you all values that will work, and recommend you the best 
    values that you should probably use (unless you're doing a high res run 
    with large buffers)
    
    eg.) python tools/peak-patch-setup-run.py 512 512
    
            If using best values:
            
            fftw_mem, 2.0 Gb        
            grid_mem, 4.01675608009 Gb
            cat_mem, 0.342726707458 Gb
            lat_mem, 0.147805440499 Gb
            s2c_mem 0.5 Gb
            
            total memory per node ~  7.00728822805 Gb
            
            ===============================
            RECOMMENDED VALUES
            ntile =  2
            nbuff =  39
            nmesh =  295
            nnodes =  1
            ===============================

----------------------------------------------------------------------
Directions for making maps from a peak patch run:
----------------------------------------------------------------------
    peak-patch/src/pks2map contains all of the mapmaking codes. 
        Maps capabilities so far include:
            pasted profiles:
                tSZ   - BBPS profiles
                kSZ   - BBPS profiles
                Kappa - BBPS profiles
            line profiles:
                CO - Tony Li et al. 2016
                HI - Villaescusa-Navarro et al. 2014

    To use pasted profiles you first need to create the BBPS tables (use model 1):
        1.) go to /src/
        2.) make make_maptable
        3.) ./bin/make_maptable maptable1 1 
    
    To make maps you must first make pks2map
        1.) go to /src/
        2.) make pks2map
        
    To run pks2map
        Usage: mpirun -np 8 pks2map <filein> <fileout> <tablefile> <profilecode> 
            [<zmin> <nside> <scramble> <center> <npix> <fov> <zmax> 
            <chihview> <model> <PSZcut>]  
            profile choices: tsz, ksz, kap, tau, tco, thi
            
        <zmin>      - Minimum redshift for halos in map
        <nside>     - healpy map nside (total pixels = 12*nside**2)
        <scramble>  - 0 for off, 1 to randomly move halo to new position on sky
        <center>    - 0 for off, 1 to centre maps on the most massive halo
        <npix>      - number of pixels on a side for flatsky 
        <fov>       - field of view for flatsky
        <zmax>      - Maximum redshift for halos in map
        <chihview>  - 0 for off, 1 to move observer to this location
        <model>     - use model 1 (AGN feedback from BBPS)
        <PSZcut>    - 0 for off, 1 to cut halos below polynomial fit to 
                      Planck 2015 selection function  
            
    For pasted profiles <tablefile>=maptable1 that you just created
    For Tco             <tablefile>=/peak-patch/tables/sfr_behroozi.dat
    for Thi             <tablefile>=anything
    
    example to make 512 CO maps from a 560Mpc box at 2.4<z<2.8:
    mpirun -np 8 ./bin/pks2map peakpatchrun_merge.pksc.13579 mapsout/CO 
        sfr_behroozi.dat tco 0.0 8 0 0 1024 4.8 3.0 0 1 0
        
----------------------------------------------------------------------
Directions for modifying the code:
----------------------------------------------------------------------

	1. Pull the latest version

	2. Create a new branch, see 
	     https://confluence.atlassian.com/display/BITBUCKET/Branching+a+Repository#BranchingaRepository-BranchaMercurialrepo)

	3. Make your changes

	4. Once you have finished and have a result you feel is ready to become 
	   part of the main trunk, close the branch by issuing a pull request, see
	     https://confluence.atlassian.com/display/BITBUCKET/Branching+a+Repository#BranchingaRepository-BranchaMercurialrepo)
	   and
	     https://confluence.atlassian.com/display/BITBUCKET/Work+with+pull+requests